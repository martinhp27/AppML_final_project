{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to a Convolutional NN - classifying handwritten numbers:\n",
    "\n",
    "For a long time, images were inaccesible to computers and ML algorithms. However, at the turn of the century very fast development - a slow revolution - happened, and the usage of Convolutional Neural Networks (CNNs) became computationally feasible. The advent of Graphical Processing Units (GPUs) also help propel the sharp rise in ML image analysis capabilities.\n",
    "\n",
    "Through first the MNIST database (1998-2004), and then the PASCAL (2005-2010) and ImageNet (2010-2017) competitions, CNN code developed to become capable of classifying the content of images. The following is an exercise, which uses the famous MNIST dataset, containing about 70000 images of handwritten digits, reduced (and anti-aliased) to 28-by-28 pixel black-and-write images.\n",
    "\n",
    "The CNN applied filters to these images, and then concatenates (i.e. \"boils down\") the images to lower resolution. This process is repeated, and finally the resulting pixel values are fed to a normal Neural Network, and made to provide an output, which lowers the loss function (in this case getting the value of the digit right). Through backpropagation, the filter values and the NN parameters are optimised, and thus in the end, the CNN becomes capable of evaluating images.\n",
    "\n",
    "The code and comments below is meant to be part illustration, part exercise.\n",
    "\n",
    "***\n",
    "\n",
    "Authors: Carl Johnsen & Troels Petersen\n",
    "\n",
    "Emails: cjjohnsen@nbi.ku.dk & petersen@nbi.dk\n",
    "\n",
    "Date: 15th of May 2021 (latest version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "split = 2000\n",
    "NoP = split\n",
    "MetaData = pd.read_csv(\"./train/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaPath = [\"camp.csv\",\"corylus.csv\",\"dust.csv\",\"grim.csv\",\"qrob.csv\",\"qsub.csv\"]\n",
    "\n",
    "MetaData = []\n",
    "todrop = ['Particle ID',\n",
    "          'Calibration Factor',\n",
    "          'Calibration Image',\n",
    "          'Camera',\n",
    "          'Capture X',\n",
    "          'Capture Y',\n",
    "          'Date',\n",
    "          'Elapsed Time',\n",
    "          'Filter Score',\n",
    "          'Image File',\n",
    "          'Image Height',\n",
    "          'Image Width',\n",
    "          'Image X',\n",
    "          'Image Y',\n",
    "          'Source Image',\n",
    "          'Time',\n",
    "          'Timestamp',\n",
    "          'imgpaths',\n",
    "          'Sample'\n",
    "         ]\n",
    "for i in range(len(MetaPath)):\n",
    "    Data = pd.read_csv(\"./train/{}\".format(MetaPath[i]))\n",
    "    Data = Data.drop(columns = todrop)\n",
    "    Data = Data[:NoP]\n",
    "    MetaData.append(Data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [\"campanian\",\"corylus\",\"dust\",\"grimsvotn\",\"qrobur\",\"qsuber\"]\n",
    "\n",
    "files1 = glob.glob('train/'+path[0]+'/*.png')\n",
    "files1 = files1[:split]\n",
    "data_camp = np.zeros((len(files1),N,N), dtype=np.uint8)\n",
    "for i in range(len(files1)):\n",
    "    im = Image.open(files1[i]).resize((N,N))\n",
    "    data_camp[i] = np.asarray(im, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "files2 = glob.glob('train/'+path[1]+'/*.png')\n",
    "files2 = files2[:split]\n",
    "data_cory = np.zeros((len(files2),N,N), dtype=np.uint8)\n",
    "for i in range(len(files2)):\n",
    "    im = Image.open(files2[i]).resize((N,N))\n",
    "    data_cory[i] = np.asarray(im, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "files3 = glob.glob('train/'+path[2]+'/*.png')\n",
    "files3 = files3[:split]\n",
    "data_dust = np.zeros((len(files3),N,N), dtype=np.uint8)\n",
    "for i in range(len(files3)):\n",
    "    im = Image.open(files3[i]).resize((N,N))\n",
    "    data_dust[i] = np.asarray(im,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "files4 = glob.glob('train/'+path[3]+'/*.png')\n",
    "files4 = files4[:split]\n",
    "data_grim = np.zeros((len(files4),N,N),dtype=np.uint8)\n",
    "for i in range(len(files4)):\n",
    "    im = Image.open(files4[i]).resize((N,N))\n",
    "    data_grim[i] = np.asarray(im, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "files5 = glob.glob('train/'+path[4]+'/*.png')\n",
    "files5 = files5[:split]\n",
    "data_qro = np.zeros((len(files5),N,N), dtype=np.uint8)\n",
    "for i in range(len(files5)):\n",
    "    im = Image.open(files5[i]).resize((N,N))\n",
    "    data_qro[i] = np.asarray(im, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "files6 = glob.glob('train/'+path[5]+'/*.png')\n",
    "files6 = files6[:split]\n",
    "data_qsu = np.zeros((len(files6),N,N), dtype=np.uint8)\n",
    "for i in range(len(files6)):\n",
    "    im = Image.open(files6[i]).resize((N,N))\n",
    "    data_qsu[i] = np.asarray(im, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp = 0*np.ones((1,split)).T\n",
    "cory = np.ones((1,split)).T\n",
    "dust = 2*np.ones((1,split)).T\n",
    "grim = 3*np.ones((1,split)).T\n",
    "qro = 4*np.ones((1,split)).T\n",
    "qso = 5*np.ones((1,split)).T\n",
    "\n",
    "labels = np.vstack([camp, cory, dust, grim, qro, qso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack((data_camp, data_cory, data_dust, data_grim, data_qro, data_qsu))\n",
    "metadata = np.vstack((MetaData[0], MetaData[1], MetaData[2], MetaData[3], MetaData[4], MetaData[5]))\n",
    "\n",
    "#mapping = dict(names)\n",
    "        \n",
    "np.savez_compressed('data_codingset_onelist_64_2000.npz', data=data, labels=labels, metadata = metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load('data_codingset_onelist_sharpness_and_contrast_512_500.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 39)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['metadata'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 512, 512)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_1.png'\n",
      " 'GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_10.png'\n",
      " 'GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_100.png' ...\n",
      " 'GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_9997.png'\n",
      " 'GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_9998.png'\n",
      " 'GRIP_raw/GRIP_3046_0_20_1/GRIP_3046_0_20_1_9999.png']\n",
      "63123\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "\n",
    "#path = [\"GRIP_3046_0_20_1\",\"GRIP_3046_20_40_1\",\"GRIP_3046_40_55_1\",\"GRIP_3136_0_20_1\", \"GRIP_3136_20_40_1\",\"GRIP_3136_40_55_1\",\"GRIP_3303_0_20_1\", \"GRIP_3303_20_40_1\", \"GRIP_3303_40_55_1\", \"GRIP_3306_0_20_1\", \"GRIP_3306_20_40_1\", \"GRIP_3306_40_55_1\"]\n",
    "\n",
    "\n",
    "files2 = glob.glob(\"GRIP_raw/GRIP_3046_0_20_1/*.png\")\n",
    "files2 = np.sort(files2, kind='quicksort')\n",
    "print(files2)\n",
    "data_raw = np.zeros((len(files2),N,N), dtype=np.uint8)\n",
    "for i in range(len(files2)):\n",
    "    im = Image.open(files2[i]).resize((N,N))\n",
    "    data_raw[i] = np.asarray(im, dtype=np.uint8)\n",
    "\n",
    "print(len(data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaPath = [\"GRIP_3046_raw.csv\"]\n",
    "\n",
    "MetaData = []\n",
    "todrop = ['Particle ID',\n",
    "          'Calibration Factor',\n",
    "          'Calibration Image',\n",
    "          'Camera',\n",
    "          'Capture X',\n",
    "          'Capture Y',\n",
    "          'Date',\n",
    "          'Elapsed Time',\n",
    "          'Filter Score',\n",
    "          'Image File',\n",
    "          'Image Height',\n",
    "          'Image Width',\n",
    "          'Image X',\n",
    "          'Image Y',\n",
    "          'Source Image',\n",
    "          'Time',\n",
    "          'Timestamp',\n",
    "          'imgpaths',\n",
    "          'Sample'\n",
    "         ]\n",
    "for i in range(len(MetaPath)):\n",
    "    Data = pd.read_csv(\"./GRIP_raw/{}\".format(MetaPath[i]))\n",
    "    Data = Data.sort_values(by='imgpaths')\n",
    "    Data = Data.drop(columns = todrop)\n",
    "    MetaData.append(Data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = data_raw\n",
    "test_meta = MetaData[0][:len(test_img)]\n",
    "\n",
    "np.savez_compressed('data_test_ice_core.npz', test_img=test_img, test_meta = test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63123, 64, 64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63123, 39)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaData[0][:len(test_img)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
